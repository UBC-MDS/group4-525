{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "certified-effort",
   "metadata": {},
   "source": [
    "# DSCI 525 - Web and Cloud Computing\n",
    "## Milestone 1: Tackling big data on your laptop\n",
    "\n",
    "### Group #4\n",
    "### Members: Heidi Ye, Junting He, Kamal MoravejJahromi, Tanmay Sharma\n",
    "\n",
    "### GitHub Repo: **https://github.com/UBC-MDS/group4-525**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-carter",
   "metadata": {},
   "source": [
    "## Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "senior-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage\n",
    "import dask.dataframe as dd\n",
    "import pyarrow.feather as feather\n",
    "import pyarrow.dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "distinct-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-trash",
   "metadata": {},
   "source": [
    "#### Note: Code across this lab has been adapted from the DSCI-525 lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-charm",
   "metadata": {},
   "source": [
    "## 1. Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "overhead-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary metadata\n",
    "article_id = 14096681  \n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"figsharerainfall/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accredited-victory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_link_only': False,\n",
       "  'name': 'daily_rainfall_2014.png',\n",
       "  'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'id': 26579150,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "  'size': 58863},\n",
       " {'is_link_only': False,\n",
       "  'name': 'environment.yml',\n",
       "  'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'id': 26579171,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "  'size': 192},\n",
       " {'is_link_only': False,\n",
       "  'name': 'README.md',\n",
       "  'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'id': 26586554,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "  'size': 5422},\n",
       " {'is_link_only': False,\n",
       "  'name': 'data.zip',\n",
       "  'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'id': 26766812,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "  'size': 814041183},\n",
       " {'is_link_only': False,\n",
       "  'name': 'get_data.py',\n",
       "  'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'computed_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'id': 26766815,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "  'size': 4113}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  \n",
    "files = data[\"files\"]             \n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-helen",
   "metadata": {},
   "source": [
    "## 2. Unzipping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "following-brooklyn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.7 s, sys: 2.73 s, total: 5.43 s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files_to_dl = [\"data.zip\"]  \n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "usual-interval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.7 s, sys: 3.32 s, total: 21.1 s\n",
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-hobby",
   "metadata": {},
   "source": [
    "## 3. Combining data CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "parliamentary-index",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>3.293256e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>1.047658e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932835</th>\n",
       "      <td>2014-12-27 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>2.951144e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932836</th>\n",
       "      <td>2014-12-28 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>2.257118e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932837</th>\n",
       "      <td>2014-12-29 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>1.204670e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932838</th>\n",
       "      <td>2014-12-30 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>2.632404e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932839</th>\n",
       "      <td>2014-12-31 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>3.431610e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1932840 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        time  lat_min  lat_max  lon_min  lon_max  \\\n",
       "0        1889-01-01 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "1        1889-01-02 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "2        1889-01-03 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "3        1889-01-04 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "4        1889-01-05 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "...                      ...      ...      ...      ...      ...   \n",
       "1932835  2014-12-27 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932836  2014-12-28 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932837  2014-12-29 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932838  2014-12-30 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932839  2014-12-31 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "\n",
       "         rain (mm/day)  \n",
       "0         3.293256e-13  \n",
       "1         0.000000e+00  \n",
       "2         0.000000e+00  \n",
       "3         0.000000e+00  \n",
       "4         1.047658e-02  \n",
       "...                ...  \n",
       "1932835   2.951144e-02  \n",
       "1932836   2.257118e-01  \n",
       "1932837   1.204670e-01  \n",
       "1932838   2.632404e-02  \n",
       "1932839   3.431610e-02  \n",
       "\n",
       "[1932840 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./figsharerainfall/ACCESS-CM2_daily_rainfall_NSW.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "governmental-contribution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 374.74 MiB, increment: 0.06 MiB\n",
      "CPU times: user 6min 44s, sys: 23.1 s, total: 7min 7s\n",
      "Wall time: 7min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "\n",
    "#Merging all the csv files\n",
    "\n",
    "files = glob.glob('figsharerainfall/*NSW.csv')\n",
    "df = pd.concat((pd.read_csv(file, index_col=0)\n",
    "                .assign(model=re.findall(r'/([^_]*)', file)[0])\n",
    "                for file in files)\n",
    "              )\n",
    "df.to_csv(\"figsharerainfall/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-blind",
   "metadata": {},
   "source": [
    "The following table summerizes the `cpu times` and `Wall time` of combining csv files for all team members.\n",
    "\n",
    "|   |  Memory Usage |  CPU Time  | Wall Time  | OS  |  Memory (RAM) |  CPUs   |\n",
    "|---|---|---|---|---|---|---|\n",
    "|  Heidi | 0.04 MiB  | 6min 45s  | 7min 10s  |  mac OS Catalina | 16 GB 3733 MHz LPDDR4X | 2 GHz Quad-Core Intel Core i5   |\n",
    "| Junting  | 0.05 MiB  |  15min 8s | 15min 36s  | mac OS Catalina  |16GB of 2400MHz DDR4 | 2.6GHz 6-core Intel Core i7   |\n",
    "| Kamal  |  0.79 MiB |  9min 20s  |  9min 43s | Windows 10 Pro  | 8 GB DDR4 SDRAM |  Intel Core i7-8650U Quad-Core  |\n",
    "|  Tanmay | 0.04 MiB  | 5min 23s  |  5min 28s |  mac OS Big Sur   | 16 GB 2667 MHz DDR4  | 2.6 GHz 6-Core Intel Core i7    |\n",
    "\n",
    "- We were able to combine the CSV files using Pandas concat method on both macOS and Windows operating systems and on the machines of all the 4 team-members.\n",
    "- `Memory usage` ranged from `0.04 MiB` to `0.79 MiB`, `CPU time` from `5min 23s` to `15min 8s` and Wall time from `5min 28s` to `15min 36s`. \n",
    "- Memory usage and CPU processing times are also impacted by background processes on the individual machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "scientific-bulgarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6G\tfigsharerainfall/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh figsharerainfall/combined_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-complexity",
   "metadata": {},
   "source": [
    "## 4. Load the combined CSV to memory and perform a simple EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "consolidated-while",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "CMCC-ESM2           3541230\n",
      "NorESM2-MM          3541230\n",
      "TaiESM1             3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-ESM4           3219300\n",
      "GFDL-CM4            3219300\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "MRI-ESM2-0          3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM5-0           1609650\n",
      "INM-CM4-8           1609650\n",
      "FGOALS-g3           1287720\n",
      "KIOST-ESM           1287720\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "NorESM2-LM           919800\n",
      "CanESM5              551880\n",
      "BCC-ESM1             551880\n",
      "Name: model, dtype: int64\n",
      "peak memory: 8403.29 MiB, increment: 4625.34 MiB\n",
      "CPU times: user 1min 6s, sys: 13.3 s, total: 1min 19s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "#loading the entire data to the memory using Pandas\n",
    "df = pd.read_csv(\"figsharerainfall/combined_data.csv\")\n",
    "print(df[\"model\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-sheet",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|   |  Memory Usage |  CPU Time  | Wall Time  | OS  |  Memory (RAM) |  CPUs   |\n",
    "|---|---|---|---|---|---|---|\n",
    "|  Heidi | 2933.94 MiB  | 54.9 s  | 1min 20s  |  mac OS Catalina | 16 GB 3733 MHz LPDDR4X | 2 GHz Quad-Core Intel Core i5   |\n",
    "| Junting  | 4149.87 MiB  |  2min 54s | 3min 11s  | mac OS Catalina  |16GB of 2400MHz DDR4 | 2.6GHz 6-core Intel Core i7   |\n",
    "| Kamal  |  1084.17 MiB |  4min 20s  |  4min 46s | Windows 10 Pro  | 8 GB DDR4 SDRAM |  Intel Core i7-8650U Quad-Core  |\n",
    "|  Tanmay | 3921.75 MiB  | 1min 1s  | 1min 4s  | mac OS Big Sur   | 16 GB 2667 MHz DDR4  | 2.6 GHz 6-Core Intel Core i7 |\n",
    "\n",
    "- We were able to load the combined CSV files using Pandas read_csv method on both macOS and Windows operating systems and on the machines of all the 4 team-members.\n",
    "- `Memory usage` ranged from `1084.17 MiB` to `4149.87 MiB`, `CPU time` from `54.9 s` to `4min 20s` and Wall time from `1min 4s` to `4min 46s`. \n",
    "- Memory usage and CPU processing times are also impacted by background processes on the individual machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "missing-copper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.244226e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.217326e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.498125e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.251282e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.270161e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time    lat_min    lat_max   lon_min   lon_max  \\\n",
       "0  1889-01-01 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "1  1889-01-02 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "2  1889-01-03 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "3  1889-01-04 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "4  1889-01-05 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "\n",
       "   rain (mm/day)            model  \n",
       "0   4.244226e-13  MPI-ESM-1-2-HAM  \n",
       "1   4.217326e-13  MPI-ESM-1-2-HAM  \n",
       "2   4.498125e-13  MPI-ESM-1-2-HAM  \n",
       "3   4.251282e-13  MPI-ESM-1-2-HAM  \n",
       "4   4.270161e-13  MPI-ESM-1-2-HAM  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "provincial-vietnamese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time              object\n",
       "lat_min          float64\n",
       "lat_max          float64\n",
       "lon_min          float64\n",
       "lon_max          float64\n",
       "rain (mm/day)    float64\n",
       "model             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking datatypes for columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-headquarters",
   "metadata": {},
   "source": [
    "### 4.1. Investigate approaches to reduce memory usage while performing the EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-cinema",
   "metadata": {},
   "source": [
    "### 4.1.1. Changing dtype of the data and loading just the columns we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alpha-professor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage with float64: 1499.23 MB\n",
      "Memory usage with float32: 749.61 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory usage with float64: {df[['lat_min','lat_max','rain (mm/day)']].memory_usage().sum() / 1e6:.2f} MB\")\n",
    "print(f\"Memory usage with float32: {df[['lat_min','lat_max','rain (mm/day)']].astype('float32', errors='ignore').memory_usage().sum() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-equivalent",
   "metadata": {},
   "source": [
    "### 4.1.2. Loading data in chunks using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "broadband-quarterly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "AWI-ESM-1-1-LR       966420\n",
      "BCC-CSM2-MR         3035340\n",
      "BCC-ESM1             551880\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "CanESM5              551880\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "FGOALS-f3-L         3219300\n",
      "FGOALS-g3           1287720\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "MIROC6              2070900\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-HR       5154240\n",
      "MPI-ESM1-2-LR        966420\n",
      "MRI-ESM2-0          3037320\n",
      "NESM3                966420\n",
      "NorESM2-LM           919800\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "TaiESM1             3541230\n",
      "dtype: int64\n",
      "peak memory: 6374.36 MiB, increment: 1354.38 MiB\n",
      "CPU times: user 1min 6s, sys: 7.75 s, total: 1min 14s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"figsharerainfall/combined_data.csv\", chunksize=10_000_000):\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-manchester",
   "metadata": {},
   "source": [
    "\n",
    "|   |  Memory Usage |  CPU Time  | Wall Time  | OS  |  Memory (RAM) |  CPUs   |\n",
    "|---|---|---|---|---|---|---|\n",
    "|  Heidi | 5967.77 MiB | 53 s  | 1min 2s |  mac OS Catalina | 16 GB 3733 MHz LPDDR4X | 2 GHz Quad-Core Intel Core i5   |\n",
    "| Junting  |2163.42  | 2min 18s | 2min 19s | mac OS Catalina  |16GB of 2400MHz DDR4 | 2.6GHz 6-core Intel Core i7   |\n",
    "| Kamal  |  1619.86 MiB |  1min 54s  |  1min 59s | Windows 10 Pro  | 8 GB DDR4 SDRAM |  Intel Core i7-8650U Quad-Core  |\n",
    "|  Tanmay |  1271.02 MiB | 55.6 s  |  56.5 s | mac OS Big Sur   | 16 GB 2667 MHz DDR4  | 2.6 GHz 6-Core Intel Core i7  |\n",
    "\n",
    "- We were able to load data in chunks using Pandas on both macOS and Windows operating systems and on the machines of all the 4 team-members..\n",
    "- `Memory usage` ranged from `1271.02 MiB` to `5967.77 MiB`, `CPU time` from `53 s` to `2min 18s` and Wall time from `56.5 s` to `2min 19s`. \n",
    "- Memory usage and CPU processing times are also impacted by background processes on the individual machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-blackberry",
   "metadata": {},
   "source": [
    "### 4.1.2. Loading data using Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "personal-residence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "TaiESM1             3541230\n",
      "NorESM2-MM          3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "MRI-ESM2-0          3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM5-0           1609650\n",
      "INM-CM4-8           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "Name: model, dtype: int64\n",
      "peak memory: 6610.67 MiB, increment: 2087.40 MiB\n",
      "CPU times: user 1min 33s, sys: 24 s, total: 1min 57s\n",
      "Wall time: 46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Using dask\n",
    "ddf = dd.read_csv('figsharerainfall/combined_data.csv')\n",
    "print(ddf[\"model\"].value_counts().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-jumping",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|   |  Memory Usage |  CPU Time  | Wall Time  | OS  |  Memory (RAM) |  CPUs   |\n",
    "|---|---|---|---|---|---|---|\n",
    "|  Heidi | 4714.38 MiB  | 1min 32s  | 42.6 s  |  mac OS Catalina | 16 GB 3733 MHz LPDDR4X | 2 GHz Quad-Core Intel Core i5   |\n",
    "| Junting  | 1817.48 MiB  | 3min 43s | 1min 19s  | mac OS Catalina  |16GB of 2400MHz DDR4 | 2.6GHz 6-core Intel Core i7   |\n",
    "| Kamal  |  1690.57 MiB|  2min 24s  |  1min 2s | Windows 10 Pro  | 8 GB DDR4 SDRAM |  Intel Core i7-8650U Quad-Core  |\n",
    "|  Tanmay |  1797.21 MiB | 1min 31s  |  34.3 s | mac OS Big Sur   | 16 GB 2667 MHz DDR4  | 2.6 GHz 6-Core Intel Core i7   |\n",
    "\n",
    "- We were able to load data using Dask on both macOS and Windows operating systems and on the machines of all the 4 team-members..\n",
    "- `Memory usage` ranged from `1690.57 MiB` to `4714.38 MiB`, `CPU time` from `1min 31s` to `3min 43s` and Wall time from `34.3 s` to `1min 19s`. \n",
    "- Memory usage and CPU processing times are also impacted by background processes on the individual machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-charter",
   "metadata": {},
   "source": [
    "### 4.2. Discuss your observations.\n",
    "\n",
    "We tried the following approaches to reduce the memory usage while performing the EDA:\n",
    "\n",
    "1. Changing dtype of the data and loading just the columns we want:\n",
    "    - We loaded only 3 out of the 5 attributes, namely, 'lat_min','lat_max', and 'rain (mm/day)'.\n",
    "    - We changed the data type of these attributes to float32 from the orignial float64.\n",
    "    - Memory usage with float32: 750.17 MB was almost half of the memory usage with float64: 1500.33 MB.\n",
    "    - This validates the hypothesis that using lower data types(float32 vs 64 in this case) leads to more efficient memory usage.\n",
    "    \n",
    "2. Loading data in chunks using Pandas\n",
    "    - We loaded the combined csv file using a chunksize=10_000_000 while performing the EDA.\n",
    "    - We observed a decline in peak memory usage from 8403.29 MiB to 6374.36 MiB.\n",
    "    - Wall Time decreased from 1min 22s to 1min 15s.\n",
    "    - We do not see a significant change in the Wall and CPU times. \n",
    "    - We hypothesize that this impact would be more pronoucned when doing more memory intensive operations in EDA and using smaller chunk sizes would further reduce the memory usage.\n",
    "    \n",
    "3. Loading data using Dask\n",
    "    - We next loaded the combined CSV using a dask object. \n",
    "    - We observed a decline in peak memory usage from 8403.29 MiB to 6610.67 MiB.\n",
    "    - Wall Time decreased significantly from 1min 22s to 46 s.\n",
    "    - We also notice that the CPU time was higher than the wall time (1min 33s vs 46s) suggesting that the CPU was performing operations in parallel. \n",
    "\n",
    "In summary, loading the entire data (combined_csv) to memory at once has the longest wall time and the highest memory usage as expected. We have looked at three different approaches to load the data more efficiently i.e. loading the entire data using pandas, loading the data in chunks, and loading data using Dask. We conclude that if we want to reduce the memory usage and the processing time, loading with Dask is the best option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-jordan",
   "metadata": {},
   "source": [
    "### 5. Perform a simple EDA in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rational-purple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 5711.06 MiB, increment: 1216.03 MiB\n",
      "CPU times: user 21.2 s, sys: 11.3 s, total: 32.5 s\n",
      "Wall time: 29.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "dataset = ds.dataset(\"figsharerainfall/combined_data.csv\", format=\"csv\")\n",
    "\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "prompt-innocent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.11 s, sys: 12.5 s, total: 17.6 s\n",
      "Wall time: 6.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# writing in feather format \n",
    "feather.write_feather(table, 'figsharerainfall/combined_data.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abandoned-function",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘arrow’\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from ‘package:utils’:\n",
      "\n",
      "    timestamp\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 27 x 2\u001b[39m\n",
      "   model                  n\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m ACCESS-CM2       1\u001b[4m9\u001b[24m\u001b[4m3\u001b[24m\u001b[4m2\u001b[24m840\n",
      "\u001b[90m 2\u001b[39m ACCESS-ESM1-5    1\u001b[4m6\u001b[24m\u001b[4m1\u001b[24m\u001b[4m0\u001b[24m700\n",
      "\u001b[90m 3\u001b[39m AWI-ESM-1-1-LR    \u001b[4m9\u001b[24m\u001b[4m6\u001b[24m\u001b[4m6\u001b[24m420\n",
      "\u001b[90m 4\u001b[39m BCC-CSM2-MR      3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m5\u001b[24m340\n",
      "\u001b[90m 5\u001b[39m BCC-ESM1          \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 6\u001b[39m CanESM5           \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 7\u001b[39m CMCC-CM2-HR4     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 8\u001b[39m CMCC-CM2-SR5     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 9\u001b[39m CMCC-ESM2        3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m10\u001b[39m EC-Earth3-Veg-LR 3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m7\u001b[24m320\n",
      "\u001b[90m# … with 17 more rows\u001b[39m\n",
      "Time difference of 15.75686 secs\n",
      "CPU times: user 11.9 s, sys: 22.6 s, total: 34.5 s\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "\n",
    "library(arrow)\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_feather(\"figsharerainfall/combined_data.feather\")\n",
    "print(class(r_table))\n",
    "library(dplyr)\n",
    "result <- r_table %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "authorized-research",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: ── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.3.0 ──\n",
      "\n",
      "R[write to console]: \u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.3     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.1.0     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.1.3     \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 1.4.0     \n",
      "\n",
      "R[write to console]: ── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 1 x 5\u001b[39m\n",
      "  median_lat_min median_lat_max median_lon_min median_lon_max median_rain\n",
      "           \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[90m1\u001b[39m            -\u001b[31m33\u001b[39m          -\u001b[31m32\u001b[39m\u001b[31m.\u001b[39m\u001b[31m0\u001b[39m           147.           148.      0.061\u001b[4m5\u001b[24m\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "library(tidyverse)\n",
    "\n",
    "r_table <- r_table %>% rename(rain_mmperday = `rain (mm/day)`)\n",
    "\n",
    "summary_table <- r_table %>%\n",
    "    drop_na() %>%\n",
    "    summarise(median_lat_min = median(lat_min),\n",
    "             median_lat_max = median(lat_max),\n",
    "             median_lon_min = median(lon_min),\n",
    "             median_lon_max = median(lon_max),\n",
    "             median_rain = median(rain_mmperday))\n",
    "    \n",
    "summary_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-gospel",
   "metadata": {},
   "source": [
    "### 5.1 Discuss why you chose this approach over others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-asian",
   "metadata": {},
   "source": [
    "### 5.1.1. Reasons to choose feather\n",
    "\n",
    "We chose `feather file format` based on the following reasons:\n",
    "\n",
    "- The `feather` file format is **faster** compared with the `parquet` file and `arrow exchange` while writing files. It writes data with lesser serialization and deserialization that would result in a higher input/output speed. As we can see in our case, the Wall time for writing the feather file is almost half of the parquet file's wall time. \n",
    "- The `feather` file format can effectively **transfer between python and R programming languages** due to the embedded API that would result in faster reading and writing data using R.\n",
    "- `Feather` also takes **fewer memories** compared with the CSV file format. We can see that the CSV file takes `5.7 GB` while the feather file format takes `1.1 GB` space. \n",
    "\n",
    "- We observed that the `partitioned.parquet` and `parquet files` take less space than the `feather` file format. However, the higher speed of writing and reading feather speeds the data queries and analysis.\n",
    "\n",
    "- `Arrow` only support some operations. The `feather` does not have this limitation.\n",
    "\n",
    "In summary, feather was selected over Parquet, Pandas Exchange, and Arrow Exchange for its comparatively high I/O speed, minimal memory on disk, and the fact that unpacking is not necessary for the data to be loaded back into RAM. Additionally, feather is relatively easy to use and is a suitable choice since the intent is not term storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-attraction",
   "metadata": {},
   "source": [
    "### 5.1.2. Challenges and discussion\n",
    "\n",
    "One of biggest challenges associated with this size of data was that computational speed became extremely slow. It was not uncommon for simple tasks to take upwards of 15 minutes. In addition, even after the data was read in, the manipulation of data was still fairly slow. One approach was to read in the data in chunks to minimize the amount of data available at one time. Although this approach may work in some use cases, it is not without its limitations. For example, there may be instances where we need full access to all the data and chunking could result in sampling the data incorrectly. Other alternatives explored in this milestone include changing the dtype of the data as well as loading in data via Dask. Again, this provided some computational savings but likely would not scale well for even larger datasets. To tackle the insufficient memory challenge certain team members ended up deleting certain files and terminating applications to make more memory available. \n",
    "\n",
    "Another challenge was that runtime did vary from machine to machine. There were instances where the same code could take three or four times longer to run depending on the system being used. This type of inconsistency makes working in this environment fairly unpredictable under tight deadlines. In this milestone, there was no apparent method of overcoming this issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-collection",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
